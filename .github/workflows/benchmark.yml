name: Benchmarks

on:
  # Run on main branch pushes
  push:
    branches: [main]
  # Manual trigger with options
  workflow_dispatch:
    inputs:
      benchmark_type:
        description: 'Benchmark type to run'
        required: true
        default: 'tool-level'
        type: choice
        options:
          - tool-level
          - action-level
          - optimizations
          - full
      include_live_tests:
        description: 'Include live API tests'
        required: false
        default: true
        type: boolean

# Don't run on every PR to save API quota
concurrency:
  group: benchmark-${{ github.ref }}
  cancel-in-progress: true

env:
  NODE_VERSION: '20'

jobs:
  # Always run compliance tests (no live API needed)
  compliance-tests:
    name: MCP Compliance Tests
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Build
        run: npm run build

      - name: Run compliance tests
        run: npm run test:compliance

  # Tool-level benchmarks (requires live API credentials)
  tool-benchmarks:
    name: Tool-Level Benchmarks
    runs-on: ubuntu-latest
    needs: compliance-tests
    # Only run if secrets are available (main branch or manual trigger)
    if: |
      github.event_name == 'workflow_dispatch' ||
      (github.event_name == 'push' && github.ref == 'refs/heads/main')

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Build
        run: npm run build

      - name: Setup test credentials
        env:
          GOOGLE_TEST_CREDENTIALS: ${{ secrets.GOOGLE_TEST_CREDENTIALS }}
        run: |
          mkdir -p tests/config
          echo "$GOOGLE_TEST_CREDENTIALS" > tests/config/test-credentials.json

      - name: Download baseline benchmarks
        uses: actions/cache@v4
        with:
          path: benchmarks/baseline.json
          key: benchmark-baseline-${{ github.base_ref || github.ref_name }}
          restore-keys: |
            benchmark-baseline-main
            benchmark-baseline-

      - name: Run tool-level benchmarks
        if: |
          github.event.inputs.benchmark_type == 'tool-level' ||
          github.event.inputs.benchmark_type == 'full' ||
          github.event_name == 'push'
        env:
          TEST_REAL_API: 'true'
          RUN_BENCHMARKS: 'true'
        run: npm run bench:tool-level

      - name: Track benchmark results & detect regressions
        run: npm run benchmarks:track
        continue-on-error: true
        id: benchmark-tracking
        env:
          BENCHMARK_THRESHOLD: '10'

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-tool-level
          path: |
            benchmark-results/
            benchmarks/latest.json
            benchmarks/history/
            benchmark-report.json
            benchmark-report.md
          retention-days: 30
          if-no-files-found: ignore

      - name: Update baseline (on main branch)
        if: github.ref == 'refs/heads/main' && steps.benchmark-tracking.outcome == 'success'
        run: |
          mkdir -p benchmarks
          cp benchmarks/latest.json benchmarks/baseline.json
          echo "✅ Baseline updated for main branch"

  # Action-level benchmarks (manual trigger only - expensive)
  action-benchmarks:
    name: Action-Level Benchmarks
    runs-on: ubuntu-latest
    needs: tool-benchmarks
    # Only run on manual trigger with action-level or full
    if: |
      github.event_name == 'workflow_dispatch' &&
      (github.event.inputs.benchmark_type == 'action-level' ||
       github.event.inputs.benchmark_type == 'full')

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Build
        run: npm run build

      - name: Setup test credentials
        env:
          GOOGLE_TEST_CREDENTIALS: ${{ secrets.GOOGLE_TEST_CREDENTIALS }}
        run: |
          mkdir -p tests/config
          echo "$GOOGLE_TEST_CREDENTIALS" > tests/config/test-credentials.json

      - name: Run action-level benchmarks
        env:
          TEST_REAL_API: 'true'
          RUN_BENCHMARKS: 'true'
        run: npm run bench:action-level
        timeout-minutes: 30

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-action-level
          path: |
            benchmark-results/
            benchmark-report.json
            benchmark-report.md
          retention-days: 30
          if-no-files-found: ignore

  # Optimization layer verification
  optimization-tests:
    name: Optimization Layer Tests
    runs-on: ubuntu-latest
    needs: compliance-tests
    if: |
      github.event_name == 'workflow_dispatch' &&
      (github.event.inputs.benchmark_type == 'optimizations' ||
       github.event.inputs.benchmark_type == 'full')

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Build
        run: npm run build

      - name: Setup test credentials
        env:
          GOOGLE_TEST_CREDENTIALS: ${{ secrets.GOOGLE_TEST_CREDENTIALS }}
        run: |
          mkdir -p tests/config
          echo "$GOOGLE_TEST_CREDENTIALS" > tests/config/test-credentials.json

      - name: Run optimization layer tests
        env:
          TEST_REAL_API: 'true'
        run: npm run bench:optimizations

      - name: Upload optimization results
        uses: actions/upload-artifact@v4
        with:
          name: optimization-test-results
          path: |
            optimization-results/
          retention-days: 30
          if-no-files-found: ignore

  # Live API tests (core + data tools)
  live-api-tests:
    name: Live API Tests
    runs-on: ubuntu-latest
    needs: compliance-tests
    if: |
      (github.event_name == 'workflow_dispatch' && github.event.inputs.include_live_tests) ||
      (github.event_name == 'push' && github.ref == 'refs/heads/main')

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Build
        run: npm run build

      - name: Setup test credentials
        env:
          GOOGLE_TEST_CREDENTIALS: ${{ secrets.GOOGLE_TEST_CREDENTIALS }}
        run: |
          mkdir -p tests/config
          echo "$GOOGLE_TEST_CREDENTIALS" > tests/config/test-credentials.json

      - name: Run live API tests (fast subset)
        env:
          TEST_REAL_API: 'true'
        run: npm run test:live:fast

  # Summary job
  benchmark-summary:
    name: Benchmark Summary
    runs-on: ubuntu-latest
    needs:
      [compliance-tests, tool-benchmarks, action-benchmarks, optimization-tests, live-api-tests]
    if: always()

    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: all-results
        continue-on-error: true

      - name: Summarize results
        run: |
          echo "## Benchmark Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Check each job status
          if [ "${{ needs.compliance-tests.result }}" == "success" ]; then
            echo "- ✅ MCP Compliance Tests: Passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "- ❌ MCP Compliance Tests: ${{ needs.compliance-tests.result }}" >> $GITHUB_STEP_SUMMARY
          fi

          if [ "${{ needs.tool-benchmarks.result }}" == "success" ]; then
            echo "- ✅ Tool-Level Benchmarks: Completed" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ needs.tool-benchmarks.result }}" == "skipped" ]; then
            echo "- ⏭️ Tool-Level Benchmarks: Skipped" >> $GITHUB_STEP_SUMMARY
          else
            echo "- ❌ Tool-Level Benchmarks: ${{ needs.tool-benchmarks.result }}" >> $GITHUB_STEP_SUMMARY
          fi

          if [ "${{ needs.action-benchmarks.result }}" == "success" ]; then
            echo "- ✅ Action-Level Benchmarks: Completed" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ needs.action-benchmarks.result }}" == "skipped" ]; then
            echo "- ⏭️ Action-Level Benchmarks: Skipped (manual trigger only)" >> $GITHUB_STEP_SUMMARY
          else
            echo "- ❌ Action-Level Benchmarks: ${{ needs.action-benchmarks.result }}" >> $GITHUB_STEP_SUMMARY
          fi

          if [ "${{ needs.optimization-tests.result }}" == "success" ]; then
            echo "- ✅ Optimization Layer Tests: Passed" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ needs.optimization-tests.result }}" == "skipped" ]; then
            echo "- ⏭️ Optimization Layer Tests: Skipped" >> $GITHUB_STEP_SUMMARY
          else
            echo "- ❌ Optimization Layer Tests: ${{ needs.optimization-tests.result }}" >> $GITHUB_STEP_SUMMARY
          fi

          if [ "${{ needs.live-api-tests.result }}" == "success" ]; then
            echo "- ✅ Live API Tests: Passed" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ needs.live-api-tests.result }}" == "skipped" ]; then
            echo "- ⏭️ Live API Tests: Skipped" >> $GITHUB_STEP_SUMMARY
          else
            echo "- ❌ Live API Tests: ${{ needs.live-api-tests.result }}" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "See artifacts for detailed benchmark results." >> $GITHUB_STEP_SUMMARY
