/**
 * ServalSheets - Prompt Registration
 *
 * Guided workflows and templates for common operations.
 *
 * @module mcp/registration/prompt-registration
 */

import type { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js';
import { z } from 'zod';
import {
  FirstOperationPromptArgsSchema,
  AnalyzeSpreadsheetPromptArgsSchema,
  TransformDataPromptArgsSchema,
  CreateReportPromptArgsSchema,
  CleanDataPromptArgsSchema,
  MigrateDataPromptArgsSchema,
  SetupBudgetPromptArgsSchema,
  ImportDataPromptArgsSchema,
  SetupCollaborationPromptArgsSchema,
  DiagnoseErrorsPromptArgsSchema,
  SafeOperationPromptArgsSchema,
  BulkImportPromptArgsSchema,
  UndoChangesPromptArgsSchema,
} from '../../schemas/prompts.js';

// ============================================================================
// PROMPTS REGISTRATION
// ============================================================================

/**
 * Registers ServalSheets prompts with the MCP server
 *
 * Prompts provide guided workflows and templates for common operations.
 *
 * @param server - McpServer instance
 */
export function registerServalSheetsPrompts(server: McpServer): void {
  // === ONBOARDING PROMPTS ===

  server.registerPrompt(
    'welcome',
    {
      description: 'ğŸ‰ Welcome to ServalSheets! Get started with this guided introduction.',
      argsSchema: {},
    },
    async () => {
      return {
        messages: [
          {
            role: 'user' as const,
            content: {
              type: 'text' as const,
              text: `ğŸ‰ Welcome to ServalSheets!

I'm your Google Sheets assistant with 21 powerful tools and 272 actions.

## ğŸš€ Quick Start
Test spreadsheet: \`1BxiMVs0XRA5nFMdKvBdBZjgmUUqptlbs74OgvE2upms\`

## ğŸ“Š Capabilities
â€¢ Data Operations: Read, write, batch operations
â€¢ Analysis: Data quality, statistics, formula audit
â€¢ AI Analysis: Pattern detection, anomalies, formula generation (via MCP Sampling)
â€¢ Formatting: Colors, fonts, conditional formatting
â€¢ Advanced: Charts, pivots, sharing, versions

## ğŸ›¡ï¸ Safety Features
â€¢ Dry-run mode, effect limits, auto-snapshots
â€¢ User confirmation for multi-step operations (via MCP Elicitation)

What would you like to do first?`,
            },
          },
        ],
      };
    }
  );

  server.registerPrompt(
    'test_connection',
    {
      description: 'ğŸ” Test your ServalSheets connection with a public spreadsheet',
      argsSchema: {},
    },
    async () => {
      return {
        messages: [
          {
            role: 'user' as const,
            content: {
              type: 'text' as const,
              text: `ğŸ” Testing ServalSheets connection!

Test spreadsheet: 1BxiMVs0XRA5nFMdKvBdBZjgmUUqptlbs74OgvE2upms

Please run these tests in order:
1. sheets_auth action: "status" â†’ Verify authentication
2. sheets_core action: "get", spreadsheetId: "1BxiMVs0XRA5nFMdKvBdBZjgmUUqptlbs74OgvE2upms" â†’ Get metadata
3. sheets_data action: "read", spreadsheetId: "1BxiMVs0XRA5nFMdKvBdBZjgmUUqptlbs74OgvE2upms", range: "Sheet1!A1:D10" â†’ Read sample data
4. sheets_analyze action: "analyze_structure", spreadsheetId: "1BxiMVs0XRA5nFMdKvBdBZjgmUUqptlbs74OgvE2upms" â†’ Analyze structure

If all tests pass, you're ready to use ServalSheets!
If auth fails, follow the authentication flow first.`,
            },
          },
        ],
      };
    }
  );

  server.registerPrompt(
    'first_operation',
    {
      description: 'ğŸ‘¶ Your first ServalSheets operation - a guided walkthrough',
      argsSchema: FirstOperationPromptArgsSchema,
    },
    async (args: Record<string, unknown>) => {
      const spreadsheetId = args['spreadsheetId'] || '1BxiMVs0XRA5nFMdKvBdBZjgmUUqptlbs74OgvE2upms';
      return {
        messages: [
          {
            role: 'user' as const,
            content: {
              type: 'text' as const,
              text: `ğŸ‘¶ First ServalSheets operation!

Spreadsheet: ${spreadsheetId}

Steps:
1. Read data: sheets_core action "get"
2. Analyze quality: sheets_analyze action "analyze_quality"
3. Get statistics: sheets_analyze action "analyze_data"
4. Format headers: sheets_format (use dryRun first!)

Safety tips: Always read before modify, use dryRun for destructive ops.`,
            },
          },
        ],
      };
    }
  );

  // === ANALYSIS PROMPTS ===

  server.registerPrompt(
    'analyze_spreadsheet',
    {
      description: 'ğŸ”¬ Comprehensive analysis of spreadsheet data quality and structure',
      argsSchema: AnalyzeSpreadsheetPromptArgsSchema,
    },
    async (args: Record<string, unknown>) => {
      return {
        messages: [
          {
            role: 'user' as const,
            content: {
              type: 'text' as const,
              text: `ğŸ”¬ Analyzing: ${args['spreadsheetId']}

Run comprehensive analysis:
1. Metadata: sheets_core action "get"
2. Data Quality: sheets_analyze action "analyze_quality"
3. Structure: sheets_analyze action "analyze_structure"
4. Formula Audit: sheets_analyze action "analyze_formulas"
5. AI Insights: sheets_analyze action "analyze_data" (uses MCP Sampling)

Provide: quality score, issues found, recommended fixes.`,
            },
          },
        ],
      };
    }
  );

  server.registerPrompt(
    'transform_data',
    {
      description: 'ğŸ”„ Transform data in a spreadsheet range with safety checks',
      argsSchema: TransformDataPromptArgsSchema,
    },
    async (args: Record<string, unknown>) => {
      return {
        messages: [
          {
            role: 'user' as const,
            content: {
              type: 'text' as const,
              text: `ğŸ”„ Transform data

Spreadsheet: ${args['spreadsheetId']}
Range: ${args['range']}
Transform: ${args['transformation']}

Workflow:
1. Read current data
2. Plan transformation
3. Confirm with user (sheets_confirm via Elicitation)
4. Execute with safety limits
5. Verify results`,
            },
          },
        ],
      };
    }
  );

  server.registerPrompt(
    'create_report',
    {
      description: 'ğŸ“ˆ Generate a formatted report from spreadsheet data',
      argsSchema: CreateReportPromptArgsSchema,
    },
    async (args: Record<string, unknown>) => {
      const reportType = args['reportType'] || 'summary';
      return {
        messages: [
          {
            role: 'user' as const,
            content: {
              type: 'text' as const,
              text: `ğŸ“ˆ Creating ${reportType} report from ${args['spreadsheetId']}

Steps:
1. Read source data
2. Create "Report" sheet
3. Add summary statistics
4. Apply formatting
${reportType === 'charts' ? '5. Add charts (use sheets_analyze to suggest best chart types)\n' : ''}
Final: Auto-resize, freeze header, add timestamp`,
            },
          },
        ],
      };
    }
  );

  server.registerPrompt(
    'clean_data',
    {
      description: 'ğŸ§¹ Clean and standardize data in a spreadsheet range',
      argsSchema: CleanDataPromptArgsSchema,
    },
    async (args: Record<string, unknown>) => {
      return {
        messages: [
          {
            role: 'user' as const,
            content: {
              type: 'text' as const,
              text: `ğŸ§¹ Cleaning data: ${args['spreadsheetId']}, range ${args['range']}

Phases:
1. Analysis: Run data quality check + AI analysis (sheets_analyze)
2. Plan: Identify duplicates, empty cells, format issues
3. Confirm: Present plan to user (sheets_confirm via Elicitation)
4. Execute: Apply with auto-snapshot
5. Validate: Compare before/after`,
            },
          },
        ],
      };
    }
  );

  // === NEW WORKFLOW PROMPTS ===

  server.registerPrompt(
    'migrate_data',
    {
      description: 'ğŸ“¦ Migrate data between spreadsheets with validation',
      argsSchema: MigrateDataPromptArgsSchema,
    },
    async (args: Record<string, unknown>) => {
      return {
        messages: [
          {
            role: 'user' as const,
            content: {
              type: 'text' as const,
              text: `ğŸ“¦ Data Migration

Source: ${args['sourceSpreadsheetId']} (${args['sourceRange']})
Target: ${args['targetSpreadsheetId']} (${args['targetRange'] || 'auto-detect'})

Migration Workflow:
1. Read source data: sheets_data action "read"
2. Validate data: Check schema, detect issues
3. Check target: Ensure compatibility
4. Plan operation: Present migration plan
5. Confirm: Use sheets_confirm for user approval
6. Execute: Copy data with transaction safety
7. Validate: Compare row counts, checksums

Safety: Creates snapshots of both sheets before migration.`,
            },
          },
        ],
      };
    }
  );

  server.registerPrompt(
    'setup_budget',
    {
      description: 'ğŸ’° Create a budget tracking spreadsheet with formulas and formatting',
      argsSchema: SetupBudgetPromptArgsSchema,
    },
    async (args: Record<string, unknown>) => {
      const budgetType = args['budgetType'] || 'personal';
      const spreadsheetId = args['spreadsheetId'];

      return {
        messages: [
          {
            role: 'user' as const,
            content: {
              type: 'text' as const,
              text: `ğŸ’° Setting up ${budgetType} budget tracker
${spreadsheetId ? `Spreadsheet: ${spreadsheetId}` : 'Creating new spreadsheet'}

Budget Setup:
1. Create structure:
   - Income sheet: Categories, amounts, dates
   - Expenses sheet: Categories, amounts, dates
   - Summary sheet: Totals, remaining, charts

2. Add formulas:
   - SUMIF for category totals
   - Date filters for monthly/yearly views
   - Remaining budget calculations

3. Format cells:
   - Currency format for amounts
   - Conditional formatting: red for overspent
   - Freeze headers

4. Add charts:
   - Pie chart: Expense breakdown
   - Line chart: Monthly trends
   - Use sheets_analyze to suggest optimal chart types

5. Setup validation:
   - Dropdowns for categories
   - Date validation
   - Positive number validation for income

Final: Apply professional formatting, add instructions sheet.`,
            },
          },
        ],
      };
    }
  );

  server.registerPrompt(
    'import_data',
    {
      description: 'ğŸ“¥ Import external data into Google Sheets with transformation',
      argsSchema: ImportDataPromptArgsSchema,
    },
    async (args: Record<string, unknown>) => {
      return {
        messages: [
          {
            role: 'user' as const,
            content: {
              type: 'text' as const,
              text: `ğŸ“¥ Data Import Workflow

Spreadsheet: ${args['spreadsheetId']}
Data source: ${args['dataSource']}
Target: ${args['targetSheet'] || 'new sheet'}

Import Steps:
1. Prepare data:
   - Parse source format (CSV, JSON, API response)
   - Validate structure
   - Clean special characters

2. Create target sheet:
   - sheets_core action "add_sheet"
   - Name appropriately

3. Import data:
   - Use sheets_data action "write" or "append"
   - Handle large datasets (batch if > 10k rows)

4. Post-import:
   - Auto-format headers
   - Freeze top row
   - Auto-resize columns
   - Add data validation

5. Quality check:
   - Run sheets_analyze "analyze_quality"
   - Verify row counts
   - Check for import errors

Pro tip: Use sheets_transaction to batch all operations into 1 API call.`,
            },
          },
        ],
      };
    }
  );

  server.registerPrompt(
    'setup_collaboration',
    {
      description: 'ğŸ‘¥ Configure sharing and permissions for team collaboration',
      argsSchema: SetupCollaborationPromptArgsSchema,
    },
    async (args: Record<string, unknown>) => {
      const role = args['role'] || 'writer';
      const collaborators = args['collaborators'] as string[];

      return {
        messages: [
          {
            role: 'user' as const,
            content: {
              type: 'text' as const,
              text: `ğŸ‘¥ Setting up collaboration

Spreadsheet: ${args['spreadsheetId']}
Adding ${collaborators.length} collaborator(s) as "${role}"

Collaboration Setup:
1. Share spreadsheet:
   ${collaborators.map((email, i) => `   ${i + 1}. sheets_collaborate action "share_add", email: "${email}", role: "${role}"`).join('\n')}

2. Setup protected ranges:
   - Lock critical formulas/headers
   - sheets_advanced action "add_protected_range"
   - Allow editors to only edit data cells

3. Add version control:
   - Create initial snapshot
   - sheets_collaborate action "version_create_snapshot"

4. Setup comments:
   - Add collaboration guidelines comment
   - sheets_collaborate action "add"

5. Configure notifications:
   - Enable edit notifications
   - Setup comment alerts

Best practices:
- Use "commenter" role for stakeholders
- Use "writer" role for team members
- Reserve "owner" role transfers for handoffs`,
            },
          },
        ],
      };
    }
  );

  server.registerPrompt(
    'diagnose_errors',
    {
      description: 'ğŸ”§ Troubleshoot and diagnose spreadsheet issues',
      argsSchema: DiagnoseErrorsPromptArgsSchema,
    },
    async (args: Record<string, unknown>) => {
      const errorDesc = args['errorDescription'] || 'general diagnostics';

      return {
        messages: [
          {
            role: 'user' as const,
            content: {
              type: 'text' as const,
              text: `ğŸ”§ Diagnosing: ${errorDesc}

Spreadsheet: ${args['spreadsheetId']}

Diagnostic Workflow:
1. Basic checks:
   - sheets_core "get": Verify access
   - Check sheet count, total cells

2. Data quality:
   - sheets_analyze "analyze_quality": Find data issues
   - sheets_analyze "analyze_formulas": Check formula errors

3. AI analysis:
   - sheets_analyze "analyze": Deep pattern analysis
   - Detect anomalies, inconsistencies

4. Performance check:
   - Check formula complexity
   - Identify slow formulas (nested VLOOKUPs)
   - Recommend ARRAYFORMULA or INDEX/MATCH

5. Structure analysis:
   - sheets_analyze "analyze_structure"
   - Check for duplicate headers
   - Verify data types per column

Common Issues:
- #REF! errors: Deleted referenced cells
- #DIV/0!: Division by zero
- #N/A: VLOOKUP not found
- Circular references: Formula refers to itself
- Performance: Too many volatile functions (NOW, RAND)

Report:
- Issue summary
- Affected ranges
- Recommended fixes
- Preventive measures`,
            },
          },
        ],
      };
    }
  );

  // Error Recovery Prompt - AI-powered troubleshooting
  server.registerPrompt(
    'recover_from_error',
    {
      description:
        'ğŸ”§ Recover from ServalSheets errors - AI-powered troubleshooting and self-healing',
      argsSchema: {
        errorCode: z.string().describe('The error code from the failed operation'),
        errorMessage: z.string().optional().describe('The full error message'),
        toolName: z.string().optional().describe('The tool that failed (e.g., sheets_data)'),
        context: z.string().optional().describe('What you were trying to do'),
      },
    },
    async (args: Record<string, unknown>) => {
      const errorCode = (args['errorCode'] as string) || 'UNKNOWN_ERROR';
      const errorMessage = (args['errorMessage'] as string) || '';
      const toolName = (args['toolName'] as string) || '';
      const context = (args['context'] as string) || '';

      const recoveryGuide: Record<string, string> = {
        INTERNAL_ERROR: `ğŸ”´ INTERNAL_ERROR - Likely Fixed in v1.3.0-hotfix.1

This was the "taskStore.isTaskCancelled is not a function" bug.

âœ… Fix Applied:
- Task cancellation bug fixed
- Rebuild: npm run build
- Restart Claude Desktop completely (Cmd+Q then relaunch)

Verification:
1. node dist/cli.js --version (should show v1.3.0)
2. Check if error persists after restart
3. Logs: ~/Library/Logs/Claude/mcp*.log

If still occurring after restart:
â€¢ Verify dist/server.js contains "this.taskStore.isTaskCancelled"
â€¢ Check Claude Desktop config path is correct
â€¢ Try: rm -rf dist && npm run build`,

        QUOTA_EXCEEDED: `âš ï¸ QUOTA_EXCEEDED - Google API Rate Limit

Immediate Actions:
1. Wait 60 seconds before retry
2. Switch to batch operations (saves 80% quota):
   sheets_data action="batch_read" ranges=["A1:B2","D1:E2"]
   Instead of: Multiple individual "read" calls

Prevention:
â€¢ Check quota: sheets_auth action="status"
â€¢ Use semantic ranges: {"semantic":{"column":"Revenue"}}
â€¢ Batch operations: batch_read, batch_write, batch_update

Recovery Time: 60 seconds per 100 requests`,

        RANGE_NOT_FOUND: `âŒ RANGE_NOT_FOUND - Sheet or Range Doesn't Exist

Diagnosis:
1. List all sheets: sheets_core action="list_sheets"
2. Check exact spelling (case-sensitive!)
3. Verify format: "SheetName!A1:D10"

Common Fixes:
â€¢ "Sheet1" not "sheet1" (case matters!)
â€¢ Include sheet name: "Data!A1:D10" not just "A1:D10"
â€¢ Check sheet wasn't deleted/renamed

Try semantic ranges: {"semantic":{"sheet":"Sales","column":"Total"}}`,

        PERMISSION_DENIED: `ğŸ”’ PERMISSION_DENIED - Authentication or Access Issue

Recovery Steps:
1. Check auth: sheets_auth action="status"
2. Re-authenticate: sheets_auth action="login"
3. Complete OAuth in browser
4. Retry operation

Access Check:
â€¢ Verify spreadsheet is shared with your account
â€¢ sheets_collaborate action="share_list" to see current access
â€¢ Request owner to share if needed

OAuth Scopes Needed:
https://www.googleapis.com/auth/spreadsheets`,

        INVALID_RANGE: `ğŸ“ INVALID_RANGE - Range Format Incorrect

Valid Formats:
âœ… "A1:D10"
âœ… "Sheet1!A1:D10"
âœ… "Sheet1!A:A" (entire column)
âœ… "Sheet1!1:1" (entire row)

Invalid Formats:
âŒ "A1-D10" (use : not -)
âŒ "A1..D10"
âŒ "SheetName A1:D10" (missing !)

Alternative: Use semantic ranges
{"semantic":{"sheet":"Data","column":"Revenue","includeHeader":false}}`,

        RATE_LIMIT_EXCEEDED: `ğŸš¦ RATE_LIMIT_EXCEEDED - Too Many Requests

Built-in Circuit Breaker Active:
â€¢ Automatic exponential backoff
â€¢ Request spacing (1-2 seconds)
â€¢ Auto-retry with delays

Your Action:
â€¢ Wait 10 seconds
â€¢ Use batch operations next time
â€¢ Let circuit breaker handle retries

Prevention: Batch operations reduce rate limit usage by 80%`,

        AUTH_EXPIRED: `ğŸ”‘ AUTH_EXPIRED - Token Expired

Auto-Recovery (Usually Works):
â€¢ Server auto-refreshes tokens
â€¢ Just retry your operation
â€¢ Token refresh happens automatically

Manual Recovery:
1. sheets_auth action="logout"
2. sheets_auth action="login"
3. Complete OAuth flow
4. Retry operation

Token Details:
â€¢ Expire after 1 hour
â€¢ Auto-refresh when possible
â€¢ Encrypted storage: GOOGLE_TOKEN_STORE_PATH`,

        NOT_FOUND: `ğŸ” NOT_FOUND - Spreadsheet Doesn't Exist

Verify ID:
â€¢ Format: 44 chars, alphanumeric plus - and _
â€¢ Get from URL: docs.google.com/spreadsheets/d/{ID}/...
â€¢ Check for typos

Find Spreadsheets:
1. List all: sheets_core action="list"
2. Create new: sheets_core action="create" title="My Sheet"

Common Issues:
â€¢ Spreadsheet deleted
â€¢ Wrong ID copied
â€¢ No access permission`,
      };

      const recovery =
        recoveryGuide[errorCode] ||
        `ğŸ”§ ${errorCode} Recovery

Tool: ${toolName || 'unknown'}
Message: ${errorMessage || 'No message provided'}
Context: ${context || 'No context provided'}

General Recovery:
1. Check tool description for correct format (see Quick Examples)
2. Verify spreadsheet ID and permissions
3. Check auth: sheets_auth action="status"
4. Review history: sheets_history
5. Try dry-run: {"safety":{"dryRun":true}}

Common Fixes:
â€¢ Auth: sheets_auth action="login"
â€¢ Quota: Wait 60s, use batch_read/batch_write
â€¢ Range: Verify with sheets_core action="get"
â€¢ Format: See tool description Quick Examples

Still Stuck?
â€¢ Logs: ~/Library/Logs/Claude/mcp*.log
â€¢ Version: node dist/cli.js --version
â€¢ Restart: Quit Claude Desktop (Cmd+Q), wait 5s, relaunch`;

      return {
        messages: [
          {
            role: 'user' as const,
            content: {
              type: 'text' as const,
              text: recovery,
            },
          },
        ],
      };
    }
  );

  // Performance Troubleshooting Prompt
  server.registerPrompt(
    'troubleshoot_performance',
    {
      description: 'âš¡ Diagnose and fix slow spreadsheet operations',
      argsSchema: {
        spreadsheetId: z.string().describe('The spreadsheet ID'),
        operation: z.string().optional().describe('What operation was slow'),
        responseTime: z.number().optional().describe('How long it took (ms)'),
      },
    },
    async (args: Record<string, unknown>) => {
      const spreadsheetId = args['spreadsheetId'] as string;
      const operation = (args['operation'] as string) || 'unknown';
      const responseTime = (args['responseTime'] as number) || 0;

      return {
        messages: [
          {
            role: 'user' as const,
            content: {
              type: 'text' as const,
              text: `âš¡ Performance Troubleshooting for ${spreadsheetId}

Operation: ${operation}
${responseTime > 0 ? `Response Time: ${responseTime}ms` : ''}

Common Performance Issues:

1. **Large Range Reads** (>10K cells)
   â€¢ Problem: Reading entire sheets instead of specific ranges
   â€¢ Fix: Use precise ranges like "A1:D100" instead of "A:Z"
   â€¢ Tool: sheets_data with exact range
   â€¢ Improvement: 80-90% faster

2. **Multiple Individual Operations**
   â€¢ Problem: 50 separate read calls instead of 1 batch
   â€¢ Fix: Use batch_read with multiple ranges
   â€¢ Tool: sheets_data action="batch_read" ranges=["A1:B10","D1:E10"]
   â€¢ Improvement: Saves 80% API quota, 3-5x faster

3. **Formula Recalculation**
   â€¢ Problem: Complex formulas with circular references
   â€¢ Fix: Use optimize_formulas prompt
   â€¢ Check: sheets_analyze action="analyze_formulas"
   â€¢ Improvement: 50-70% faster calculations

4. **Network Latency**
   â€¢ Problem: Too many round trips to Google API
   â€¢ Fix: Bundle operations in sheets_transaction
   â€¢ Improvement: Single API call instead of N calls

5. **Unoptimized Queries**
   â€¢ Problem: Reading full sheet to find one value
   â€¢ Fix: Use sheets_data action="find_replace" with criteria
   â€¢ Improvement: 95% faster than scanning

Diagnostic Steps:

1. Check range size:
   â€¢ sheets_core action="get" â†’ See total rows/columns
   â€¢ If >10K cells, reduce range

2. Enable profiling:
   â€¢ Add timing: const start = Date.now()
   â€¢ Measure each operation
   â€¢ Identify slowest step

3. Review recent operations:
   â€¢ sheets_history action="list" limit=10
   â€¢ Look for repeated calls

4. Analyze data structure:
   â€¢ sheets_analyze action="analyze_performance"
   â€¢ Get optimization suggestions

Quick Fixes by Operation Type:

â€¢ sheets_data read â†’ Use batch_read, exact ranges
â€¢ sheets_format â†’ Batch in sheets_transaction
â€¢ sheets_analyze â†’ Limit to <10K cells
â€¢ sheets_visualize â†’ Reduce source range size
â€¢ sheets_visualize â†’ Limit data points to <1000

Apply fixes and retest!`,
            },
          },
        ],
      };
    }
  );

  // Data Quality Fix Prompt
  server.registerPrompt(
    'fix_data_quality',
    {
      description: 'ğŸ” Identify and fix data quality issues',
      argsSchema: {
        spreadsheetId: z.string().describe('The spreadsheet ID'),
        range: z.string().describe('Range to analyze'),
        issues: z.string().optional().describe('Known issues'),
      },
    },
    async (args: Record<string, unknown>) => {
      const spreadsheetId = args['spreadsheetId'] as string;
      const range = args['range'] as string;
      const issues = (args['issues'] as string) || 'auto-detect';

      return {
        messages: [
          {
            role: 'user' as const,
            content: {
              type: 'text' as const,
              text: `ğŸ” Data Quality Analysis for ${spreadsheetId}
Range: ${range}
${issues !== 'auto-detect' ? `Known Issues: ${issues}` : ''}

Step 1: Detect Issues
Run: sheets_analyze action="analyze_data" spreadsheetId="${spreadsheetId}" range="${range}"

Common Data Quality Problems:

1. **Empty Cells in Required Columns**
   â€¢ Detection: Check for null/empty values
   â€¢ Fix: sheets_data action="find_replace" find="" â†’ Fill or remove rows
   â€¢ Prevention: Add validation rules

2. **Duplicate Headers**
   â€¢ Detection: Count unique values in row 1
   â€¢ Fix: sheets_core action="update_sheet" â†’ Rename duplicates
   â€¢ Prevention: Validate on import

3. **Inconsistent Formats**
   â€¢ Detection: Mixed date formats, number formats
   â€¢ Fix: sheets_format action="set_number_format" format="YYYY-MM-DD"
   â€¢ Prevention: Apply format before data entry

4. **Invalid Values**
   â€¢ Detection: Negative ages, future dates, out-of-range numbers
   â€¢ Fix: sheets_data action="find_replace" with valid values
   â€¢ Prevention: sheets_format action="set_data_validation"

5. **Extra Whitespace**
   â€¢ Detection: Leading/trailing spaces
   â€¢ Fix: Use TRIM formula or sheets_data action="find_replace"
   â€¢ Prevention: Input validation

Cleanup Workflow:

1. Analyze:
   sheets_analyze action="analyze_data" range="${range}"

2. Fix empty cells:
   â€¢ Delete: sheets_dimensions action="delete_rows"
   â€¢ Fill: sheets_data action="write" with default values

3. Standardize formats:
   â€¢ Dates: sheets_format format="yyyy-mm-dd"
   â€¢ Currency: sheets_format format="$#,##0.00"
   â€¢ Percentages: sheets_format format="0.00%"

4. Remove duplicates:
   â€¢ Find: sheets_data action="find_replace"
   â€¢ Mark or delete duplicates

5. Add validation:
   â€¢ sheets_format action="set_data_validation" type="LIST"
   â€¢ Prevent future bad data

6. Verify:
   â€¢ Re-run sheets_analyze
   â€¢ Check quality score improved

After cleanup, consider:
â€¢ Create snapshot: sheets_collaborate action="version_create_snapshot"
â€¢ Document changes: sheets_collaborate action="comment_add"`,
            },
          },
        ],
      };
    }
  );

  // Formula Optimization Prompt
  server.registerPrompt(
    'optimize_formulas',
    {
      description: 'ğŸ“Š Optimize slow or inefficient formulas',
      argsSchema: {
        spreadsheetId: z.string().describe('The spreadsheet ID'),
        range: z.string().optional().describe('Range with slow formulas'),
      },
    },
    async (args: Record<string, unknown>) => {
      const spreadsheetId = args['spreadsheetId'] as string;
      const range = (args['range'] as string) || 'entire sheet';

      return {
        messages: [
          {
            role: 'user' as const,
            content: {
              type: 'text' as const,
              text: `ğŸ“Š Formula Optimization for ${spreadsheetId}
${range !== 'entire sheet' ? `Range: ${range}` : ''}

Step 1: Audit Formulas
Run: sheets_analyze action="analyze_formulas" spreadsheetId="${spreadsheetId}"

Common Formula Performance Issues:

1. **VLOOKUP** (Slow for large datasets)
   â€¢ Problem: O(n) lookup, scans entire column
   â€¢ Fix: Replace with INDEX/MATCH
   â€¢ Before: =VLOOKUP(A2,Data!A:D,3,FALSE)
   â€¢ After: =INDEX(Data!C:C,MATCH(A2,Data!A:A,0))
   â€¢ Improvement: 60% faster

2. **Array Formulas** (Resource intensive)
   â€¢ Problem: Recalculates entire array on every change
   â€¢ Fix: Split into individual cell formulas
   â€¢ Or: Use FILTER() with specific criteria
   â€¢ Improvement: 70% faster

3. **Volatile Functions** (Recalculate constantly)
   â€¢ Problem: NOW(), RAND(), INDIRECT() recalc on every edit
   â€¢ Fix: Replace with static values or manual triggers
   â€¢ NOW() â†’ Use timestamp in cell, update manually
   â€¢ INDIRECT() â†’ Use direct cell references
   â€¢ Improvement: 80% less recalculation

4. **Circular References**
   â€¢ Problem: Formulas referencing themselves
   â€¢ Detection: sheets_analyze shows circular_refs
   â€¢ Fix: Break cycle by moving calculation to different cell
   â€¢ Improvement: Prevents infinite loops

5. **Nested IFs** (Hard to read and slow)
   â€¢ Problem: =IF(A1>10,IF(A1>20,"High","Medium"),"Low")
   â€¢ Fix: Use IFS() or lookup table
   â€¢ After: =IFS(A1>20,"High",A1>10,"Medium",TRUE,"Low")
   â€¢ Improvement: More readable, 30% faster

Optimization Workflow:

1. Find slow formulas:
   â€¢ sheets_analyze action="analyze_formulas"
   â€¢ Look for: VLOOKUP, array formulas, volatile functions

2. Test performance:
   â€¢ Time recalculation (Ctrl+Alt+Shift+F9 in Sheets)
   â€¢ Identify slowest formulas

3. Replace VLOOKUP:
   â€¢ Find all: sheets_data action="find_replace" find="VLOOKUP"
   â€¢ Replace manually with INDEX/MATCH pattern

4. Simplify array formulas:
   â€¢ Convert to individual formulas
   â€¢ Or use more efficient array operations

5. Remove volatile functions:
   â€¢ Replace NOW() with manual timestamp
   â€¢ Replace INDIRECT() with direct references

6. Verify improvements:
   â€¢ Re-run formula audit
   â€¢ Test recalculation speed

Formula Best Practices:

â€¢ Use named ranges (easier to read and maintain)
â€¢ Avoid full column references (A:A) when possible
â€¢ Cache lookup results instead of repeated calculations
â€¢ Use FILTER() instead of complex IF arrays
â€¢ Break complex formulas into intermediate cells

After optimization:
â€¢ Document changes in comments
â€¢ Create version snapshot
â€¢ Monitor performance over time`,
            },
          },
        ],
      };
    }
  );

  // Bulk Import Workflow Prompt
  server.registerPrompt(
    'bulk_import_data',
    {
      description: 'ğŸ“¥ Efficiently import large datasets',
      argsSchema: {
        spreadsheetId: z.string().describe('Target spreadsheet ID'),
        dataSize: z.number().optional().describe('Approximate row count'),
        dataSource: z.string().optional().describe('Source description'),
      },
    },
    async (args: Record<string, unknown>) => {
      const spreadsheetId = args['spreadsheetId'] as string;
      const dataSize = (args['dataSize'] as number) || 0;
      const dataSource = (args['dataSource'] as string) || 'external source';

      return {
        messages: [
          {
            role: 'user' as const,
            content: {
              type: 'text' as const,
              text: `ğŸ“¥ Bulk Data Import Workflow for ${spreadsheetId}
Source: ${dataSource}
${dataSize > 0 ? `Estimated Rows: ${dataSize}` : ''}

Optimal Import Strategy:

${
  dataSize > 10000
    ? `âš ï¸ LARGE DATASET (${dataSize} rows)
Use chunked imports with transactions`
    : ''
}

Step 1: Prepare Target Sheet
1. Create or clear target sheet:
   sheets_core action="add_sheet" title="Import_${new Date().toISOString().split('T')[0]}"

2. Setup structure:
   â€¢ Headers: sheets_data action="write" range="A1:Z1" values=[["Col1","Col2",...]]
   â€¢ Format headers: sheets_format range="A1:Z1" bold=true backgroundColor="#4285F4"
   â€¢ Freeze: sheets_dimensions action="freeze_rows" count=1

Step 2: Validate Source Data
1. Check data quality before import
2. Remove: Empty rows, invalid characters, duplicates
3. Standardize: Date formats, number formats, text encoding

Step 3: Import Data (Choose Strategy)

**Strategy A: Small Dataset (<1000 rows)**
â€¢ Single batch write:
  sheets_data action="batch_write" ranges=["A2:Z1001"] values=[...]

**Strategy B: Medium Dataset (1K-10K rows)**
â€¢ Transaction with chunks:
  sheets_transaction action="begin"
  For each chunk of 1000 rows:
    sheets_transaction action="add_operation" operation=write
  sheets_transaction action="commit"

**Strategy C: Large Dataset (>10K rows)**
â€¢ Multiple transactions:
  For every 5000 rows:
    Begin transaction â†’ Write 5 chunks of 1000 â†’ Commit
    Wait 2 seconds between transactions

Step 4: Post-Import Processing

1. Auto-resize columns:
   sheets_dimensions action="auto_resize" dimension="COLUMNS"

2. Apply formatting:
   â€¢ Currency columns: sheets_format format="$#,##0.00"
   â€¢ Date columns: sheets_format format="yyyy-mm-dd"
   â€¢ Conditional formatting: sheets_format for visual cues

3. Add validation rules:
   â€¢ Dropdowns: sheets_format action="set_data_validation" type="LIST"
   â€¢ Range validation: For numeric columns

4. Create summary:
   â€¢ Row count, column count
   â€¢ Add to first sheet or separate "Summary" sheet

Step 5: Verification

1. Data quality check:
   sheets_analyze action="analyze_data" range="A1:Z${dataSize || 10000}"

2. Spot check:
   â€¢ First 10 rows: sheets_data range="A2:Z11"
   â€¢ Last 10 rows: Check end of data
   â€¢ Random sample: Middle rows

3. Create checkpoint:
   sheets_collaborate action="version_create_snapshot" description="After ${dataSource} import"

Performance Tips:

â€¢ Batch size: 1000 rows optimal for balance of speed/reliability
â€¢ Use batch_write not individual writes (80% faster)
â€¢ Wait 2s between large transactions (avoid rate limits)
â€¢ Format after data import (faster than formatting during)
â€¢ Create indexes with named ranges for quick access

Error Recovery:

â€¢ If import fails mid-way:
  1. sheets_history action="list" - Find last successful operation
  2. sheets_transaction action="rollback" - Undo partial import
  3. Resume from last checkpoint

â€¢ If data quality issues found:
  Use fix_data_quality prompt for cleanup

Import complete! âœ…`,
            },
          },
        ],
      };
    }
  );

  // ===Safety Workflow Prompts ===

  server.registerPrompt(
    'safe_operation',
    {
      description:
        'ğŸ›¡ï¸ Execute destructive operations safely with dry-run â†’ confirm â†’ execute workflow',
      argsSchema: SafeOperationPromptArgsSchema,
    },
    async (args: Record<string, unknown>) => {
      const operationType = args['operationType'] as string;
      const affectedRange = (args['affectedRange'] as string) || 'auto-detect';

      return {
        messages: [
          {
            role: 'user' as const,
            content: {
              type: 'text' as const,
              text: `ğŸ›¡ï¸ Safe ${operationType} Workflow
Spreadsheet: ${args['spreadsheetId']}
${affectedRange !== 'auto-detect' ? `Range: ${affectedRange}` : ''}

âš ï¸ CRITICAL: ${operationType} operations are PERMANENT. Follow this workflow:

Phase 1: DRY-RUN (Preview)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Preview what will happen:      â”‚
â”‚    {"safety":{"dryRun":true}}     â”‚
â”‚                                    â”‚
â”‚ 2. Review the preview output      â”‚
â”‚ 3. Verify affected ranges         â”‚
â”‚ 4. Check estimated impact         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Phase 2: IMPACT ANALYSIS
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Check dependencies:            â”‚
â”‚    sheets_quality action="analyze_impact" â”‚
â”‚                                    â”‚
â”‚ 2. Find affected formulas         â”‚
â”‚ 3. List dependent charts          â”‚
â”‚ 4. Identify broken references     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Phase 3: USER CONFIRMATION ${affectedRange.includes(':') && affectedRange.split(':').length > 1 ? '(REQUIRED)' : ''}
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MUST use sheets_confirm for:      â”‚
â”‚ â€¢ ${operationType === 'delete' ? 'Deleting >10 rows/columns' : ''}${operationType === 'bulk_update' ? 'Updating >100 cells' : ''}${operationType === 'format' ? 'Formatting >100 cells' : ''}${operationType === 'formula' ? 'Changing complex formulas' : ''}  â”‚
â”‚                                    â”‚
â”‚ Build confirmation plan:          â”‚
â”‚ {                                 â”‚
â”‚   "action": "request",            â”‚
â”‚   "plan": {                       â”‚
â”‚     "title": "${operationType} operation",â”‚
â”‚     "steps": [                    â”‚
â”‚       {                          â”‚
â”‚         "description": "...",    â”‚
â”‚         "risk": "high",          â”‚
â”‚         "isDestructive": true,   â”‚
â”‚         "canUndo": true          â”‚
â”‚       }                          â”‚
â”‚     ]                            â”‚
â”‚   }                              â”‚
â”‚ }                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Phase 4: SNAPSHOT (Undo Capability)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Create restore point:             â”‚
â”‚ {"safety":{"createSnapshot":true}}â”‚
â”‚                                    â”‚
â”‚ OR use sheets_collaborate:           â”‚
â”‚ sheets_collaborate action="version_create_snapshot"â”‚
â”‚ description="Before ${operationType}" â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Phase 5: EXECUTE SAFELY
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Remove dryRun flag             â”‚
â”‚ 2. Keep createSnapshot:true       â”‚
â”‚ 3. Execute operation              â”‚
â”‚ 4. Verify results immediately     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Phase 6: VERIFY
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Run sheets_analyze to verify  â”‚
â”‚ 2. Spot-check affected ranges     â”‚
â”‚ 3. Test dependent formulas        â”‚
â”‚ 4. Confirm no broken references   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

UNDO if needed:
â€¢ sheets_history action="undo"
â€¢ sheets_collaborate action="version_restore_revision" revisionId="..."
â€¢ sheets_transaction action="rollback" (if in transaction)

Remember: DRY-RUN â†’ IMPACT â†’ CONFIRM â†’ SNAPSHOT â†’ EXECUTE â†’ VERIFY`,
            },
          },
        ],
      };
    }
  );

  server.registerPrompt(
    'bulk_import',
    {
      description: 'ğŸ“¦ Import large datasets efficiently using transactions (80% quota savings)',
      argsSchema: BulkImportPromptArgsSchema,
    },
    async (args: Record<string, unknown>) => {
      const rowCount = (args['rowCount'] as number) || 0;
      const targetSheet = (args['targetSheet'] as string) || 'new sheet';

      return {
        messages: [
          {
            role: 'user' as const,
            content: {
              type: 'text' as const,
              text: `ğŸ“¦ Bulk Import Workflow
Spreadsheet: ${args['spreadsheetId']}
Data: ${args['dataDescription']}
Target: ${targetSheet}
${rowCount > 0 ? `Rows: ~${rowCount}` : ''}

ğŸš€ TRANSACTION WORKFLOW (Required for efficiency):

Step 1: BEGIN Transaction
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ sheets_transaction action="begin" â”‚
â”‚ spreadsheetId="${args['spreadsheetId']}"   â”‚
â”‚ autoRollback=true                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â†’ Returns: transactionId="tx_..."

Step 2: QUEUE Operations
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ For each data chunk (1000 rows):  â”‚
â”‚                                    â”‚
â”‚ sheets_transaction action="queue" â”‚
â”‚ transactionId="tx_..."            â”‚
â”‚ operation={                        â”‚
â”‚   tool: "sheets_data",          â”‚
â”‚   action: "write",                â”‚
â”‚   params: {                       â”‚
â”‚     range: "A2:Z1001",           â”‚
â”‚     values: [[...]]              â”‚
â”‚   }                              â”‚
â”‚ }                                 â”‚
â”‚                                    â”‚
â”‚ Repeat for each chunk...          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 3: COMMIT All Operations
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ sheets_transaction action="commit"â”‚
â”‚ transactionId="tx_..."            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â†’ Executes ALL operations in 1 API call!

Performance Benefits:
âœ… 1 API call instead of ${rowCount > 0 ? Math.ceil(rowCount / 1000) : 'N'} calls
âœ… 80-95% quota savings
âœ… 10x faster execution
âœ… Atomic execution (all-or-nothing)
âœ… Auto-rollback on failure

Optimal Strategy by Size:

${rowCount < 1000 ? 'ğŸ“˜ SMALL (<1000 rows): Single transaction, all data at once' : ''}
${rowCount >= 1000 && rowCount < 10000 ? 'ğŸ“— MEDIUM (1K-10K): Single transaction, chunked into 1000-row writes' : ''}
${rowCount >= 10000 ? 'ğŸ“• LARGE (>10K): Multiple transactions, 5000 rows each, 2s pause between' : ''}

Complete Example:
\`\`\`
# 1. Begin
sheets_transaction begin â†’ tx_123

# 2. Queue writes (repeat for each chunk)
sheets_transaction queue tx_123 operation=write range=A2:Z1001
sheets_transaction queue tx_123 operation=write range=A1002:Z2001
sheets_transaction queue tx_123 operation=write range=A2002:Z3001

# 3. Commit (1 API call executes all)
sheets_transaction commit tx_123
â†’ 3 operations, 1 API call, 66% quota saved!
\`\`\`

Error Recovery:
â€¢ If commit fails â†’ Auto-rollback (no partial writes)
â€¢ If need to abort â†’ sheets_transaction rollback tx_123
â€¢ Spreadsheet stays consistent (atomic guarantee)

After Import:
1. sheets_dimensions action="auto_resize" (columns)
2. sheets_format (apply formatting)
3. sheets_collaborate action="version_create_snapshot" (checkpoint)
4. sheets_analyze action="analyze_quality" (verify)

Transaction = Speed + Safety + Atomicity`,
            },
          },
        ],
      };
    }
  );

  server.registerPrompt(
    'undo_changes',
    {
      description: 'âª Undo recent changes using version history or operation history',
      argsSchema: UndoChangesPromptArgsSchema,
    },
    async (args: Record<string, unknown>) => {
      const changeDesc = (args['changeDescription'] as string) || 'recent changes';

      return {
        messages: [
          {
            role: 'user' as const,
            content: {
              type: 'text' as const,
              text: `âª Undo: ${changeDesc}
Spreadsheet: ${args['spreadsheetId']}

ğŸ” Step 1: Identify What to Undo

Option A: Recent Operations (Last 100 ops)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ sheets_history action="list"      â”‚
â”‚ spreadsheetId="${args['spreadsheetId']}"   â”‚
â”‚ limit=20                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â†’ Shows recent operations with IDs

Option B: Version History (Google's snapshots)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ sheets_collaborate action="version_list_revisions"â”‚
â”‚ spreadsheetId="${args['spreadsheetId']}"   â”‚
â”‚ limit=10                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â†’ Shows saved snapshots

âª Step 2: Choose Undo Method

Method 1: HISTORY ROLLBACK (Precise)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Best for: Specific operation undo  â”‚
â”‚                                    â”‚
â”‚ sheets_history action="undo"      â”‚
â”‚ spreadsheetId="${args['spreadsheetId']}"   â”‚
â”‚ operationId="op_12345"            â”‚
â”‚                                    â”‚
â”‚ OR revert to specific point:      â”‚
â”‚ sheets_history action="revert_to" â”‚
â”‚ operationId="op_12345"            â”‚
â”‚ (undoes everything after this)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Method 2: VERSION RESTORE (Full restore)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Best for: Major undo, "go back"   â”‚
â”‚                                    â”‚
â”‚ sheets_collaborate action="version_restore_revision"  â”‚
â”‚ spreadsheetId="${args['spreadsheetId']}"   â”‚
â”‚ revisionId="rev_abc123"           â”‚
â”‚                                    â”‚
â”‚ Restores ENTIRE spreadsheet to    â”‚
â”‚ that point in time                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Method 3: TRANSACTION ROLLBACK (In-progress)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Best for: Active transaction       â”‚
â”‚                                    â”‚
â”‚ sheets_transaction action="rollback"â”‚
â”‚ transactionId="tx_123"            â”‚
â”‚                                    â”‚
â”‚ Undoes uncommitted operations     â”‚
â”‚ (only works before commit)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ” Decision Tree:

Q: Is the change from the last few operations?
  âœ… Use sheets_history action="undo"

Q: Do you need to go back >100 operations?
  âœ… Use sheets_collaborate action="version_restore_revision"

Q: Is a transaction still in progress?
  âœ… Use sheets_transaction action="rollback"

Q: Want to undo specific cells only?
  âœ… Manually write old values back:
     1. Get old values from history/version
     2. sheets_data action="write" with old values

ğŸ“‹ Verification After Undo:

1. Check the change was undone:
   sheets_core action="get"
   sheets_data action="read" range="affected_range"

2. Verify no broken references:
   sheets_analyze action="analyze_formulas"

3. Check data quality:
   sheets_analyze action="analyze_quality"

âš ï¸ Important Notes:

â€¢ History: Keeps last 100 operations (~24 hours typical)
â€¢ Versions: Google saves automatically (~every 30 min when editing)
â€¢ Manual snapshots: Created with createSnapshot:true in safety params
â€¢ Transaction rollback: Only before commit

ğŸ›¡ï¸ Prevent Need for Undo:

ALWAYS use safety workflow for destructive ops:
1. {"safety":{"dryRun":true}} â†’ Preview
2. sheets_confirm â†’ User approval
3. {"safety":{"createSnapshot":true}} â†’ Backup
4. Execute â†’ With safety guards
5. Verify â†’ Check results

Then you'll always have an easy undo path!`,
            },
          },
        ],
      };
    }
  );

  // === CONFIRMATION GUIDE PROMPTS ===

  server.registerPrompt(
    'when_to_confirm',
    {
      description: 'ğŸ›¡ï¸ Learn when and how to request user confirmation before operations',
      argsSchema: {},
    },
    async () => {
      return {
        messages: [
          {
            role: 'user' as const,
            content: {
              type: 'text' as const,
              text: `ğŸ›¡ï¸ When to Request User Confirmation

This guide tells you EXACTLY when to use sheets_confirm.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ”´ ALWAYS CONFIRM (No exceptions):

1. DELETING SHEETS
   â€¢ Any sheets_core action="delete_sheet" call
   â€¢ Say: "This will permanently delete the sheet and all its data."

2. DELETING ROWS (>10)
   â€¢ sheets_dimensions action="delete_rows" with count > 10
   â€¢ Say: "I found {N} rows to delete. Want to see which ones first?"

3. DELETING COLUMNS (>3)
   â€¢ sheets_dimensions action="delete_columns" with count > 3
   â€¢ Say: "Deleting {N} columns may affect formulas. Proceed?"

4. CLEARING DATA (>100 cells)
   â€¢ sheets_data action="clear" on large ranges
   â€¢ Say: "This will erase {N} cells of data. Continue?"

5. LARGE WRITES (>500 cells)
   â€¢ sheets_data action="write" with >500 cells
   â€¢ Say: "I'll update {N} cells. Create a backup first?"

6. MULTI-STEP OPERATIONS (3+ steps)
   â€¢ When your plan has 3 or more operations
   â€¢ Use sheets_confirm to show the plan

7. SHARING/PERMISSIONS
   â€¢ Any sheets_collaborate call
   â€¢ Say: "This will give {email} access to your data."

8. ANYTHING USER DIDN'T EXPLICITLY REQUEST
   â€¢ If you're doing something as a side effect
   â€¢ Always ask first

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸŸ¡ SUGGEST CONFIRMATION (Offer, don't require):

â€¢ 50-500 cell modifications
â€¢ Formatting large ranges
â€¢ Adding formulas to existing data
â€¢ Sorting/filtering operations
â€¢ Import operations

How to offer: "I'll update {N} cells. Want me to show a preview first?"

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âœ… NO CONFIRMATION NEEDED:

â€¢ All read operations (read, get, list, find)
â€¢ Single cell updates user explicitly asked for
â€¢ Small writes (<50 cells) user requested
â€¢ Analysis operations
â€¢ Getting statistics

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“‹ HOW TO USE sheets_confirm:

Step 1: Build a plan
\`\`\`json
{
  "action": "request",
  "plan": {
    "title": "Delete Empty Rows",
    "description": "Remove 47 rows with no data",
    "steps": [
      {
        "stepNumber": 1,
        "description": "Delete 47 empty rows",
        "tool": "sheets_dimensions",
        "action": "delete_rows",
        "risk": "high",
        "isDestructive": true,
        "canUndo": true
      }
    ],
    "willCreateSnapshot": true,
    "additionalWarnings": ["Cannot be undone without snapshot"]
  }
}
\`\`\`

Step 2: Call sheets_confirm

Step 3: Check response
â€¢ If approved â†’ Execute the plan
â€¢ If declined â†’ Abort and explain what was avoided
â€¢ If modified â†’ Adjust plan per user feedback

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ’¡ CONVERSATIONAL ALTERNATIVES:

Instead of formal sheets_confirm, you can also ask naturally:

â€¢ "Before I delete these 15 rows, want to see which ones?"
â€¢ "This will clear 200 cells. Should I create a backup first?"
â€¢ "I found 3 duplicates. Remove them?"

But for ANYTHING destructive or large, sheets_confirm is preferred
because it shows a proper UI and creates an audit trail.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ¯ GOLDEN RULES:

1. Deletion = ALWAYS confirm
2. >100 cells = ALWAYS confirm
3. Sharing = ALWAYS confirm
4. Multi-step = ALWAYS show plan
5. Small explicit requests = No confirmation
6. When uncertain = ASK
7. Offer snapshots for destructive ops
8. Offer dryRun when user seems unsure`,
            },
          },
        ],
      };
    }
  );

  server.registerPrompt(
    'confirmation_examples',
    {
      description: 'ğŸ“ Examples of good vs bad confirmation behavior',
      argsSchema: {},
    },
    async () => {
      return {
        messages: [
          {
            role: 'user' as const,
            content: {
              type: 'text' as const,
              text: `ğŸ“ Confirmation Examples: Good vs Bad

Learn from these examples of proper confirmation behavior.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

EXAMPLE 1: Delete rows with empty status

User: "Delete all rows where status is empty"

âŒ BAD (Don't do this):
"Done! I deleted 23 rows."

âœ… GOOD:
"I found 23 rows with empty status. Before I delete them:
1. Want to see which rows they are?
2. I'll create a snapshot so you can undo if needed.

Ready to proceed?"

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

EXAMPLE 2: Clear a column

User: "Clear column D"

âŒ BAD:
"Cleared column D."

âœ… GOOD:
"Column D has 847 cells with data. Clearing them all will:
â€¢ Erase all values in D2:D848
â€¢ Break any formulas referencing this column

Should I:
a) Show you what's in column D first?
b) Create a snapshot then clear?
c) Clear just empty rows?"

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

EXAMPLE 3: Simple cell update

User: "Change cell A1 to 'Hello'"

âŒ BAD (Over-confirming):
"Are you absolutely sure you want to change A1? This will 
overwrite the existing value. Please confirm..."

âœ… GOOD:
"Done! Changed A1 to 'Hello'."

(No confirmation needed for small, explicit requests)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

EXAMPLE 4: Create a CRM (multi-step)

User: "Set up a CRM for me"

âŒ BAD:
"Creating CRM... done! I made 6 sheets and added formulas."

âœ… GOOD:
"I'll create a CRM with:
â€¢ ğŸ“Š Dashboard - Overview and metrics
â€¢ ğŸ‘¥ Contacts - Your contact list
â€¢ ğŸ¢ Companies - Business accounts
â€¢ ğŸ’° Deals - Sales pipeline
â€¢ ğŸ“ Activities - Task tracking
â€¢ âš™ï¸ Settings - Dropdown values

This involves about 15 operations. Here's my plan:
[Shows plan via sheets_confirm]

Ready to create it?"

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

EXAMPLE 5: Share spreadsheet

User: "Share this with john@example.com"

âŒ BAD:
"Shared with john@example.com as editor."

âœ… GOOD:
"I'll share with john@example.com. What permission level?
â€¢ ğŸ‘ï¸ Viewer - Can only view
â€¢ ğŸ’¬ Commenter - Can view and comment
â€¢ âœï¸ Editor - Can make changes

Also, should I send them a notification email?"

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

EXAMPLE 6: User says "just do it"

User: "Just format all the headers, don't ask me"

âœ… CORRECT BEHAVIOR:
"Got it! Formatting headers now..."
(Respect user's preference for less confirmation)

But then track this preference for future operations.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

KEY TAKEAWAYS:

1. Big operations â†’ Always confirm with details
2. Small explicit requests â†’ Just do it
3. Destructive operations â†’ Offer snapshot + confirm
4. Multi-step â†’ Show the plan first
5. Respect "just do it" preferences
6. When in doubt â†’ Ask nicely`,
            },
          },
        ],
      };
    }
  );

  // === ADVANCED WORKFLOW PROMPTS ===

  server.registerPrompt(
    'advanced_data_migration',
    {
      description:
        'ğŸš€ Advanced multi-sheet, multi-spreadsheet data migration with transformation and validation',
      argsSchema: {
        sourceSpreadsheetId: z.string().describe('Source spreadsheet ID'),
        targetSpreadsheetId: z.string().describe('Target spreadsheet ID'),
        migrationType: z
          .enum(['full', 'incremental', 'selective'])
          .optional()
          .describe('Migration type: full, incremental, or selective'),
        transformations: z.string().optional().describe('Data transformations to apply'),
      },
    },
    async (args: Record<string, unknown>) => {
      const migrationType = (args['migrationType'] as string) || 'full';
      const hasTransformations = Boolean(args['transformations']);

      return {
        messages: [
          {
            role: 'user' as const,
            content: {
              type: 'text' as const,
              text: `ğŸš€ Advanced Data Migration Workflow

Source: ${args['sourceSpreadsheetId']}
Target: ${args['targetSpreadsheetId']}
Type: ${migrationType}
${hasTransformations ? `Transformations: ${args['transformations']}` : ''}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“‹ PHASE 1: DISCOVERY & PLANNING

1. Analyze Source Structure:
   sheets_core action="list_sheets" â†’ Get all sheets
   For each sheet:
     sheets_data action="read" range="{sheet}!A1:Z1" â†’ Get headers
     sheets_analyze action="analyze_structure" â†’ Understand data types
     sheets_analyze action="analyze_quality" â†’ Check quality issues

2. Analyze Target Structure:
   sheets_core action="get" spreadsheetId=target
   Identify: Matching sheets, conflicts, missing sheets

3. Build Migration Plan:
   â€¢ Sheet mapping (source â†’ target)
   â€¢ Column mapping (handle renames/reordering)
   â€¢ Data type conversions needed
   â€¢ Validation rules to preserve
   â€¢ Formulas to update (cell references)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âš™ï¸  PHASE 2: PRE-MIGRATION VALIDATION

1. Compatibility Check:
   â€¢ Verify target has capacity (row/column limits)
   â€¢ Check for naming conflicts
   â€¢ Validate data type compatibility
   â€¢ Identify potential data loss scenarios

2. Impact Analysis:
   sheets_quality action="analyze_impact" operation="migrate"
   â€¢ Find dependent sheets/formulas
   â€¢ Identify broken references after migration
   â€¢ Calculate migration complexity

3. Create Safety Net:
   sheets_collaborate action="version_create_snapshot" spreadsheetId=target
   description="Before ${migrationType} migration from ${args['sourceSpreadsheetId']}"

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ”„ PHASE 3: DATA MIGRATION (Choose Strategy)

${
  migrationType === 'full'
    ? `
**FULL MIGRATION Strategy:**

For each source sheet:

  Step 1: Prepare Target Sheet
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Create or clear target sheet:     â”‚
  â”‚ sheets_core action="add_sheet"         â”‚
  â”‚   OR                              â”‚
  â”‚ sheets_data action="clear"      â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  Step 2: Migrate Data with Transaction
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ sheets_transaction action="begin" â”‚
  â”‚                                    â”‚
  â”‚ Queue operations:                  â”‚
  â”‚ 1. Write headers (transformed)    â”‚
  â”‚ 2. Write data in 1000-row chunks  â”‚
  â”‚ 3. Copy formatting rules          â”‚
  â”‚ 4. Recreate data validation       â”‚
  â”‚ 5. Update formulas (refs)         â”‚
  â”‚                                    â”‚
  â”‚ sheets_transaction action="commit"â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  Step 3: Verify Migration
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ â€¢ Row count match                  â”‚
  â”‚ â€¢ Column count match              â”‚
  â”‚ â€¢ Spot-check sample data          â”‚
  â”‚ â€¢ Verify formulas work            â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
`
    : migrationType === 'incremental'
      ? `
**INCREMENTAL MIGRATION Strategy:**

Migrate only NEW or CHANGED data:

Step 1: Find Delta
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Identify changes since last sync: â”‚
â”‚ â€¢ Compare row counts              â”‚
â”‚ â€¢ Check modification timestamps   â”‚
â”‚ â€¢ Hash data for change detection  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 2: Sync Changes
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ For NEW rows:                      â”‚
â”‚   sheets_data action="append"   â”‚
â”‚                                    â”‚
â”‚ For MODIFIED rows:                â”‚
â”‚   sheets_composite action="bulk_update"â”‚
â”‚   keyColumn="ID"                  â”‚
â”‚                                    â”‚
â”‚ For DELETED rows:                 â”‚
â”‚   sheets_dimensions action="delete_rows"â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
`
      : `
**SELECTIVE MIGRATION Strategy:**

Migrate specific ranges or conditions:

Step 1: Define Selection Criteria
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â€¢ Specific ranges                  â”‚
â”‚ â€¢ Filter conditions               â”‚
â”‚ â€¢ Date ranges                     â”‚
â”‚ â€¢ Data quality thresholds         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 2: Extract and Transform
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ For each selection:               â”‚
â”‚   sheets_data action="read"     â”‚
â”‚   Apply transformations           â”‚
â”‚   Validate data                   â”‚
â”‚   sheets_data action="write" targetâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
`
}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ§ª PHASE 4: POST-MIGRATION VALIDATION

1. Data Integrity Checks:
   âœ“ Row counts match (source vs target)
   âœ“ No data loss (spot-check samples)
   âœ“ Formulas working (no #REF! errors)
   âœ“ Data types preserved
   âœ“ Formatting preserved (if needed)

2. Quality Analysis:
   sheets_analyze action="analyze_quality" spreadsheetId=target
   â€¢ Check for: empty cells, duplicates, outliers
   â€¢ Compare quality scores: source vs target

3. Formula Verification:
   sheets_analyze action="analyze_formulas"
   â€¢ Verify no broken references
   â€¢ Check formula complexity unchanged
   â€¢ Test key calculations

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“Š PHASE 5: RECONCILIATION & REPORTING

1. Generate Migration Report:
   {
     "summary": {
       "sheetsProcessed": N,
       "rowsMigrated": N,
       "formulasUpdated": N,
       "dataQuality": "PASS/FAIL",
       "duration": "MM:SS"
     },
     "issues": [
       {"type": "WARNING", "sheet": "Sheet1", "description": "..."}
     ],
     "recommendations": [...]
   }

2. Create Verification Sheet:
   sheets_core action="add_sheet" title="Migration_Verification"
   Add summary table with:
   â€¢ Sheet-by-sheet comparison
   â€¢ Row count deltas
   â€¢ Quality scores
   â€¢ Issues found

3. Final Snapshot:
   sheets_collaborate action="version_create_snapshot"
   description="After ${migrationType} migration - SUCCESS"

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ¯ SUCCESS CRITERIA

âœ… All sheets migrated without data loss
âœ… Formula references updated correctly
âœ… Data quality maintained or improved
âœ… No broken validations or formatting
âœ… Verification report generated
âœ… Snapshots created for rollback

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âš ï¸  ROLLBACK PROCEDURE (If Issues Found)

1. sheets_collaborate action="version_restore_revision" revisionId="pre-migration"
2. Review migration report for root cause
3. Fix issues in migration logic
4. Re-run migration with corrections

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ’¡ PRO TIPS

â€¢ Use transactions for atomicity (all-or-nothing)
â€¢ Migrate during low-usage hours
â€¢ Test migration on copy first
â€¢ Keep source spreadsheet until verified
â€¢ Document column mappings for future reference
â€¢ Monitor API quota usage during large migrations
â€¢ Use batch operations (80% faster than individual)

Ready to execute migration! ğŸš€`,
            },
          },
        ],
      };
    }
  );

  server.registerPrompt(
    'performance_audit',
    {
      description:
        'âš¡ Comprehensive spreadsheet performance audit with optimization recommendations',
      argsSchema: {
        spreadsheetId: z.string().describe('Spreadsheet ID to audit'),
        focusAreas: z
          .array(z.string())
          .optional()
          .describe('Focus areas: formulas, data_size, api_usage, caching, structure'),
      },
    },
    async (args: Record<string, unknown>) => {
      const focusAreas = (args['focusAreas'] as string[]) || ['all'];

      return {
        messages: [
          {
            role: 'user' as const,
            content: {
              type: 'text' as const,
              text: `âš¡ Performance Audit for ${args['spreadsheetId']}
Focus: ${focusAreas.join(', ')}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ” PHASE 1: BASELINE ASSESSMENT

1. Spreadsheet Structure:
   sheets_core action="get"

   Analyze:
   â€¢ Total sheets: N
   â€¢ Total cells: rows Ã— columns Ã— sheets
   â€¢ File size estimate: (cells Ã— 100 bytes)
   â€¢ Sheets with >10K rows (performance risk)
   â€¢ Sheets with >26 columns (Z+) (complexity indicator)

2. Formula Complexity Analysis:
   sheets_analyze action="analyze_formulas"

   Identify:
   â€¢ Total formulas: N
   â€¢ Volatile functions: NOW(), RAND(), INDIRECT() (expensive!)
   â€¢ Array formulas: {...} (recalc intensive)
   â€¢ VLOOKUP usage: (suggest INDEX/MATCH)
   â€¢ Circular references: (performance killer)
   â€¢ Nested IF depth: >3 levels (hard to maintain)
   â€¢ External references: OtherSheet!A1 (cross-sheet dependencies)

3. Data Quality Check:
   sheets_analyze action="analyze_quality"

   Find:
   â€¢ Empty rows/columns: (wasted space)
   â€¢ Duplicate data: (use sheets_composite deduplicate)
   â€¢ Inconsistent data types: (causes formula errors)
   â€¢ Large text fields: >1000 chars (slow rendering)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“Š PHASE 2: PERFORMANCE BOTTLENECK DETECTION

**Formula Performance Issues:**

ğŸ”´ CRITICAL (Fix Immediately):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â€¢ Volatile functions in large ranges              â”‚
â”‚   Problem: NOW() in 10,000 cells = 10K recalcs   â”‚
â”‚   Fix: Replace with static timestamp             â”‚
â”‚                                                    â”‚
â”‚ â€¢ Circular references                             â”‚
â”‚   Problem: Infinite calculation loops            â”‚
â”‚   Fix: Break cycle by moving calc to new cell    â”‚
â”‚                                                    â”‚
â”‚ â€¢ Array formulas on full columns (A:A)           â”‚
â”‚   Problem: Evaluates all 1M rows                 â”‚
â”‚   Fix: Use specific range A1:A1000               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸŸ¡ HIGH PRIORITY (Fix Soon):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â€¢ VLOOKUP on large datasets                       â”‚
â”‚   Before: =VLOOKUP(A2,Data!A:D,3,FALSE)         â”‚
â”‚   After:  =INDEX(Data!C:C,MATCH(A2,Data!A:A,0)) â”‚
â”‚   Gain: 60% faster                               â”‚
â”‚                                                    â”‚
â”‚ â€¢ Nested IF statements (>3 levels)               â”‚
â”‚   Before: =IF(A1>10,IF(A1>20,"H","M"),"L")      â”‚
â”‚   After:  =IFS(A1>20,"H",A1>10,"M",TRUE,"L")    â”‚
â”‚   Gain: More readable, 30% faster               â”‚
â”‚                                                    â”‚
â”‚ â€¢ INDIRECT for dynamic references                â”‚
â”‚   Problem: Recalculates on every change         â”‚
â”‚   Fix: Use direct cell references or named rangesâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

**Data Structure Issues:**

ğŸ”´ CRITICAL:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â€¢ Sheets with >100K cells                         â”‚
â”‚   Solution: Split into multiple sheets           â”‚
â”‚             Use pivot tables for summaries       â”‚
â”‚                                                    â”‚
â”‚ â€¢ Excessive empty rows (trailing data)           â”‚
â”‚   Solution: sheets_dimensions action="delete_rows"â”‚
â”‚             Clean up data range                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âš™ï¸  PHASE 3: API USAGE OPTIMIZATION

Check Current Efficiency:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Resource: cache://stats                           â”‚
â”‚                                                    â”‚
â”‚ Current Stats:                                    â”‚
â”‚ â€¢ API calls: N                                    â”‚
â”‚ â€¢ Cache hit rate: X%                              â”‚
â”‚ â€¢ Efficiency gain: Y%                             â”‚
â”‚                                                    â”‚
â”‚ Target: >50% efficiency gain                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Optimization Recommendations:

1ï¸âƒ£  BATCHING (20-40% savings)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âŒ DON'T: Multiple individual reads                â”‚
â”‚    sheets_data action="read" range="A1:B10"     â”‚
â”‚    sheets_data action="read" range="D1:E10"     â”‚
â”‚    sheets_data action="read" range="G1:H10"     â”‚
â”‚    Result: 3 API calls                            â”‚
â”‚                                                    â”‚
â”‚ âœ… DO: Single batch read                          â”‚
â”‚    sheets_data action="batch_read" ranges=[    â”‚
â”‚      "A1:B10", "D1:E10", "G1:H10"               â”‚
â”‚    ]                                              â”‚
â”‚    Result: 1 API call (66% savings!)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

2ï¸âƒ£  TRANSACTIONS (80-95% savings for bulk)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âŒ DON'T: Individual write operations              â”‚
â”‚    For 100 rows: 100 API calls                   â”‚
â”‚                                                    â”‚
â”‚ âœ… DO: Transaction-wrapped bulk operation         â”‚
â”‚    sheets_transaction action="begin"             â”‚
â”‚    For chunks: queue write operations            â”‚
â”‚    sheets_transaction action="commit"            â”‚
â”‚    Result: 1 API call (99% savings!)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

3ï¸âƒ£  CACHING (15-30% savings)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â€¢ Metadata (sheet list, properties) cached       â”‚
â”‚ â€¢ Frequently-read ranges cached                  â”‚
â”‚ â€¢ Cache auto-invalidates on writes               â”‚
â”‚                                                    â”‚
â”‚ Tip: Don't repeatedly call sheets_core getâ”‚
â”‚      Results cached automatically                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“ˆ PHASE 4: STRUCTURE OPTIMIZATION

Run Full Analysis:
sheets_analyze action="analyze_structure"

Common Issues & Fixes:

1. Duplicate Headers:
   Problem: Multiple columns named "Date"
   Fix: sheets_core action="update_sheet" â†’ Rename to unique names

2. Mixed Data Types in Columns:
   Problem: "Age" column has numbers and text
   Fix: sheets_analyze â†’ Find inconsistencies â†’ Clean data

3. Unnecessary Sheets:
   Problem: 20 sheets, only 5 used
   Fix: Archive or delete unused sheets

4. Inefficient Layouts:
   Problem: Wide sheets (50+ columns)
   Fix: Consider pivot tables or transposed layout

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“‹ PHASE 5: GENERATE AUDIT REPORT

Create Audit Report Sheet:
sheets_core action="add_sheet" title="Performance_Audit_Report"

Report Sections:

1. EXECUTIVE SUMMARY
   â€¢ Overall Performance Score: X/100
   â€¢ Critical Issues: N
   â€¢ Est. Speed Improvement: Y%
   â€¢ Est. API Savings: Z%

2. FORMULA ANALYSIS
   â€¢ Total Formulas: N
   â€¢ Volatile Functions: N (replace with static)
   â€¢ VLOOKUPs to Optimize: N (convert to INDEX/MATCH)
   â€¢ Circular References: N (fix immediately)

3. STRUCTURE RECOMMENDATIONS
   â€¢ Sheets to Split: [List]
   â€¢ Empty Rows to Remove: N
   â€¢ Duplicate Columns: [List]

4. API EFFICIENCY
   â€¢ Current: X% efficiency
   â€¢ Potential: Y% efficiency (+Z% improvement)
   â€¢ Recommendation: Use batching + transactions

5. ACTION PLAN (Prioritized)
   Priority 1 (Critical):
     â€¢ Fix circular references
     â€¢ Remove volatile functions from large ranges
     â€¢ Split sheets >100K cells

   Priority 2 (High):
     â€¢ Convert VLOOKUP to INDEX/MATCH
     â€¢ Implement batching for repeated operations
     â€¢ Clean up empty rows

   Priority 3 (Medium):
     â€¢ Optimize data structure
     â€¢ Add named ranges
     â€¢ Implement caching strategy

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ’¡ QUICK WINS (Implement These First)

1. Enable Batching: Use batch_read instead of individual reads
   Impact: 20-40% API savings immediately

2. Fix Volatile Functions: Replace NOW() with manual timestamps
   Impact: 80% recalculation reduction

3. Delete Empty Rows: sheets_dimensions action="delete_rows"
   Impact: Faster load times, cleaner data

4. Use Transactions: Wrap bulk operations
   Impact: 80-95% API savings for bulk ops

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Audit complete! Review findings and implement recommendations. ğŸ¯`,
            },
          },
        ],
      };
    }
  );

  server.registerPrompt(
    'batch_optimizer',
    {
      description: 'ğŸ”„ Convert inefficient individual operations to optimized batch operations',
      argsSchema: {
        operationType: z
          .enum(['read', 'write', 'update', 'format', 'mixed'])
          .describe('Type of operations to optimize'),
        operationCount: z.number().optional().describe('Number of individual operations'),
        spreadsheetId: z.string().describe('Spreadsheet ID'),
      },
    },
    async (args: Record<string, unknown>) => {
      const operationType = args['operationType'] as string;
      const operationCount = (args['operationCount'] as number) || 10;

      return {
        messages: [
          {
            role: 'user' as const,
            content: {
              type: 'text' as const,
              text: `ğŸ”„ Batch Operation Optimizer

Spreadsheet: ${args['spreadsheetId']}
Operation Type: ${operationType}
Current: ${operationCount} individual operations

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“Š CURRENT INEFFICIENCY ANALYSIS

${
  operationType === 'read'
    ? `
**READING ${operationCount} RANGES INDIVIDUALLY**

âŒ Current Approach (INEFFICIENT):
\`\`\`
For each of ${operationCount} ranges:
  sheets_data action="read" range="..."
  Wait for response
  Process data
\`\`\`

Cost Analysis:
â€¢ API Calls: ${operationCount}
â€¢ Time: ~${operationCount * 0.3}s (${operationCount}Ã— 300ms average)
â€¢ Quota Usage: ${operationCount} read requests
â€¢ Failure Risk: ${operationCount} opportunities for errors

âœ… Optimized Approach (BATCH READ):
\`\`\`
sheets_data action="batch_read" ranges=[
  "Sheet1!A1:B10",
  "Sheet1!D1:E10",
  ...${operationCount} ranges
]
\`\`\`

Savings Analysis:
â€¢ API Calls: 1 (${operationCount - 1} saved!)
â€¢ Time: ~0.5s (${(operationCount * 0.3 - 0.5).toFixed(1)}s faster)
â€¢ Quota Savings: ${(((operationCount - 1) / operationCount) * 100).toFixed(0)}%
â€¢ Failure Risk: 1 call to monitor

ğŸ¯ IMPROVEMENT: ${(((operationCount - 1) / operationCount) * 100).toFixed(0)}% fewer API calls
`
    : operationType === 'write'
      ? `
**WRITING ${operationCount} RANGES INDIVIDUALLY**

âŒ Current Approach (INEFFICIENT):
\`\`\`
For each of ${operationCount} ranges:
  sheets_data action="write"
    range="..."
    values=[...]
\`\`\`

Cost: ${operationCount} API calls

âœ… Option 1: Batch Write (Moderate Improvement)
\`\`\`
sheets_data action="batch_write" data=[
  {range: "A1:B10", values: [...]},
  {range: "D1:E10", values: [...]},
  ...${operationCount} writes
]
\`\`\`
Savings: ${(((operationCount - 1) / operationCount) * 100).toFixed(0)}% fewer API calls

âœ… Option 2: Transaction (BEST - Atomic)
\`\`\`
sheets_transaction action="begin"
  â†’ transactionId

For each write:
  sheets_transaction action="queue"
    transactionId=...
    operation={write details}

sheets_transaction action="commit"
  transactionId=...
\`\`\`

Benefits:
â€¢ API Calls: 3 total (begin + queueÃ—N + commit = 1 actual API call)
â€¢ **Atomicity**: All succeed or all fail (no partial writes)
â€¢ **Rollback**: Auto-rollback on error
â€¢ **Performance**: 80-95% faster
â€¢ **Quota Savings**: ${Math.floor(((operationCount - 3) / operationCount) * 100)}%

ğŸ¯ IMPROVEMENT: ${Math.floor(((operationCount - 3) / operationCount) * 100)}% fewer API calls + atomicity guarantee
`
      : operationType === 'update'
        ? `
**UPDATING ${operationCount} CELLS/RANGES**

âŒ Current Approach:
Multiple individual update calls

âœ… Optimized: Use sheets_composite bulk_update

sheets_composite action="bulk_update"
  spreadsheetId="${args['spreadsheetId']}"
  sheet="Sheet1"
  keyColumn="ID"
  updates=[
    {ID: 1, Name: "New Name", Status: "Active"},
    {ID: 2, Price: 99.99},
    ...${operationCount} updates
  ]

Features:
â€¢ Updates by key column (like SQL UPDATE WHERE)
â€¢ Only modifies specified fields
â€¢ Preserves other columns
â€¢ Single API call
â€¢ Automatic row matching

ğŸ¯ IMPROVEMENT: ${operationCount} operations â†’ 1 API call
`
        : operationType === 'format'
          ? `
**FORMATTING ${operationCount} RANGES**

âŒ Current Approach:
Multiple sheets_format calls

âœ… Optimized: Transaction-wrapped formatting

sheets_transaction action="begin"

Queue all format operations:
  sheets_transaction action="queue" operation={
    tool: "sheets_format",
    action: "set_background_color",
    params: {range: "A1:B10", color: {red: 1}}
  }
  ...${operationCount} format operations

sheets_transaction action="commit"

Result: All ${operationCount} formats applied in 1 API call!

ğŸ¯ IMPROVEMENT: 80-95% faster, atomic formatting
`
          : `
**MIXED OPERATIONS (${operationCount} TOTAL)**

For mixed operation types, use Transaction Manager:

âœ… Optimized Workflow:

Step 1: Begin Transaction
\`\`\`
sheets_transaction action="begin"
  spreadsheetId="${args['spreadsheetId']}"
  autoRollback=true
â†’ Returns: transactionId="tx_..."
\`\`\`

Step 2: Queue All Operations
\`\`\`
// Read operation
sheets_transaction action="queue"
  transactionId="tx_..."
  operation={
    tool: "sheets_data",
    action: "read",
    params: {range: "A1:B10"}
  }

// Write operation
sheets_transaction action="queue"
  transactionId="tx_..."
  operation={
    tool: "sheets_data",
    action: "write",
    params: {range: "D1:E10", values: [...]}
  }

// Format operation
sheets_transaction action="queue"
  transactionId="tx_..."
  operation={
    tool: "sheets_format",
    action: "set_bold",
    params: {range: "A1:B1", bold: true}
  }

...repeat for all ${operationCount} operations
\`\`\`

Step 3: Commit Transaction
\`\`\`
sheets_transaction action="commit"
  transactionId="tx_..."
\`\`\`

Result: All ${operationCount} operations execute in single API call!

Benefits:
â€¢ ${operationCount} â†’ 1 API call (${Math.floor(((operationCount - 1) / operationCount) * 100)}% reduction)
â€¢ Atomic execution (all-or-nothing)
â€¢ Auto-rollback on failure
â€¢ Preserves operation order
â€¢ ${Math.floor(((operationCount - 1) / operationCount) * 100)}% quota savings

ğŸ¯ IMPROVEMENT: ${Math.floor(((operationCount - 1) / operationCount) * 100)}% API reduction + atomicity
`
}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“‹ IMPLEMENTATION GUIDE

${
  operationType === 'read'
    ? `
**Step-by-Step: Convert to Batch Read**

1. Collect all ranges to read:
   ranges = ["Sheet1!A1:B10", "Sheet1!D1:E10", ...]

2. Single batch read call:
   sheets_data action="batch_read"
     spreadsheetId="${args['spreadsheetId']}"
     ranges=ranges

3. Process results:
   Response includes all range data in one object
   Access via response.valueRanges[i]

Example:
\`\`\`json
{
  "action": "batch_read",
  "spreadsheetId": "${args['spreadsheetId']}",
  "ranges": [
    "Sheet1!A1:B10",
    "Sheet1!D1:E10",
    "Sheet2!A1:Z100"
  ]
}
\`\`\`

Result: 1 API call instead of ${operationCount}!
`
    : operationType === 'write'
      ? `
**Step-by-Step: Convert to Transaction**

1. Begin Transaction:
\`\`\`json
{
  "tool": "sheets_transaction",
  "action": "begin",
  "spreadsheetId": "${args['spreadsheetId']}",
  "autoRollback": true
}
\`\`\`
â†’ Save transactionId from response

2. Queue Each Write:
\`\`\`json
{
  "tool": "sheets_transaction",
  "action": "queue",
  "transactionId": "tx_...",
  "operation": {
    "tool": "sheets_data",
    "action": "write",
    "params": {
      "spreadsheetId": "${args['spreadsheetId']}",
      "range": "A1:B10",
      "values": [[1, 2], [3, 4], ...]
    }
  }
}
\`\`\`
Repeat for all ${operationCount} writes

3. Commit Transaction:
\`\`\`json
{
  "tool": "sheets_transaction",
  "action": "commit",
  "transactionId": "tx_..."
}
\`\`\`

Done! All ${operationCount} writes in 1 API call with atomicity.
`
      : `
**Step-by-Step: Transaction for Mixed Operations**

1. Plan operations:
   â€¢ Group by dependency (reads before writes that use read data)
   â€¢ List all operations

2. Begin transaction:
   sheets_transaction action="begin"

3. Queue in order:
   For each operation:
     sheets_transaction action="queue" operation={...}

4. Commit:
   sheets_transaction action="commit"

5. Verify:
   Check response for success
   All operations executed atomically
`
}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ’¡ BEST PRACTICES

1. **Batch Size**: Aim for 10-100 operations per batch
   â€¢ Too small: Less efficiency gain
   â€¢ Too large: Longer request timeout risk

2. **Error Handling**: Transactions auto-rollback on failure
   â€¢ No partial state
   â€¢ Retry entire transaction

3. **Progress Tracking**: For large batches (>100 ops)
   â€¢ Break into multiple transactions
   â€¢ Commit after each batch
   â€¢ Track progress externally

4. **Testing**: Always test with small batch first
   â€¢ Verify operation ordering
   â€¢ Check data correctness
   â€¢ Then scale to full batch size

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“Š EXPECTED RESULTS

Before Optimization:
â€¢ API Calls: ${operationCount}
â€¢ Time: ~${(operationCount * 0.3).toFixed(1)}s
â€¢ Failure Risk: ${operationCount} points
â€¢ Quota Usage: ${operationCount} requests

After Optimization:
â€¢ API Calls: ${operationType === 'write' || operationType === 'mixed' ? '3 (1 effective)' : '1'}
â€¢ Time: ~0.5-1s
â€¢ Failure Risk: 1 point
â€¢ Quota Saved: ${Math.floor(((operationCount - (operationType === 'write' || operationType === 'mixed' ? 3 : 1)) / operationCount) * 100)}%

**Performance Gain: ${Math.floor(((operationCount - (operationType === 'write' || operationType === 'mixed' ? 3 : 1)) / operationCount) * 100)}% fewer API calls** âš¡

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Ready to optimize! Convert your operations now. ğŸš€`,
            },
          },
        ],
      };
    }
  );

  // === ULTIMATE ANALYSIS TOOL PROMPTS (P2) ===

  server.registerPrompt(
    'ultimate_analysis',
    {
      description: 'ğŸ§  Ultimate Analysis Tool - Intelligent routing for data analysis',
      argsSchema: {
        spreadsheetId: z.string().describe('Spreadsheet ID from URL (required)'),
      },
    },
    async (args: Record<string, unknown>) => {
      return {
        messages: [
          {
            role: 'user' as const,
            content: {
              type: 'text' as const,
              text: `ğŸ§  Ultimate Analysis Tool

Spreadsheet: ${args['spreadsheetId']}

## ğŸ¯ INTELLIGENT ROUTING

The analysis tool automatically selects the optimal execution path:

**Fast Path** (<10K cells)
â€¢ Traditional statistics
â€¢ Completes in <2s
â€¢ Best for: Quick summaries, small datasets

**AI Path** (10K-50K cells)
â€¢ LLM-powered insights via MCP Sampling
â€¢ Completes in <15s
â€¢ Best for: Deep insights, pattern detection, recommendations

**Streaming Path** (>50K cells)
â€¢ Task-based chunked processing
â€¢ Async execution with progress tracking
â€¢ Best for: Large datasets, comprehensive analysis

## ğŸ“Š USAGE

Basic Analysis:
\`\`\`json
{
  "tool": "sheets_analyze",
  "action": "analyze_data",
  "spreadsheetId": "${args['spreadsheetId']}",
  "analysisTypes": ["summary", "quality", "patterns"]
}
\`\`\`

The router will:
1. Fetch metadata (tier 1, ~0.3s)
2. Determine dataset size
3. Select optimal path (fast/AI/streaming)
4. Execute analysis
5. Store result as \`analyze://results/{id}\`

## ğŸ” ANALYSIS TYPES

â€¢ **summary**: Overall data summary
â€¢ **patterns**: Pattern recognition
â€¢ **anomalies**: Outlier detection
â€¢ **trends**: Trend analysis
â€¢ **quality**: Data quality assessment
â€¢ **correlations**: Relationship discovery
â€¢ **recommendations**: Actionable suggestions

## ğŸ’¡ TIPS

1. **Small datasets (<10K)**: Fast path is sufficient
2. **Medium datasets (10K-50K)**: AI path provides best insights
3. **Large datasets (>50K)**: Streaming path handles without timeout
4. **Follow-up analysis**: Reference previous results via \`analyze://results/{id}\`

Ready to analyze! What insights do you need? ğŸš€`,
            },
          },
        ],
      };
    }
  );

  server.registerPrompt(
    'create_visualization',
    {
      description: 'ğŸ“Š Create charts/pivots with AI recommendations and user confirmation',
      argsSchema: {
        spreadsheetId: z.string().describe('Spreadsheet ID from URL (required)'),
      },
    },
    async (args: Record<string, unknown>) => {
      return {
        messages: [
          {
            role: 'user' as const,
            content: {
              type: 'text' as const,
              text: `ğŸ“Š Create Visualization with AI

Spreadsheet: ${args['spreadsheetId']}

## ğŸ¨ WORKFLOW (3 Steps)

**Step 1: Get Recommendations**
\`\`\`json
{
  "tool": "sheets_analyze",
  "action": "suggest_visualization",
  "spreadsheetId": "${args['spreadsheetId']}",
  "range": { "a1": "Sheet1!A1:D100" }
}
\`\`\`

AI will analyze your data and suggest:
â€¢ Best chart types (LINE, BAR, PIE, SCATTER, etc.)
â€¢ Optimal data ranges
â€¢ Axis configurations
â€¢ Pivot table dimensions

**Step 2: User Confirmation (Automatic)**

When you create a chart/pivot, MCP Elicitation will prompt:
\`\`\`
âš ï¸ Create Chart

You are about to create a LINE chart in spreadsheet ${args['spreadsheetId']}.

The chart will use data from range A1:D100.

This will modify the spreadsheet by adding a new chart object.

[ Confirm ] [ Cancel ]
\`\`\`

**Step 3: Create**
\`\`\`json
{
  "tool": "sheets_analyze",
  "action": "create_recommended_chart",
  "spreadsheetId": "${args['spreadsheetId']}",
  "chartType": "LINE",
  "range": { "a1": "Sheet1!A1:D100" }
}
\`\`\`

## ğŸ“ˆ CHART TYPES AVAILABLE

â€¢ LINE: Time series, trends
â€¢ BAR: Comparisons, rankings
â€¢ COLUMN: Category comparisons
â€¢ PIE: Part-to-whole relationships
â€¢ SCATTER: Correlation analysis
â€¢ AREA: Volume over time
â€¢ COMBO: Multiple metrics
â€¢ STEPPED_AREA: Staged progress

## ğŸ”„ PIVOT TABLES

For pivot tables:
\`\`\`json
{
  "tool": "sheets_analyze",
  "action": "create_recommended_pivot",
  "spreadsheetId": "${args['spreadsheetId']}",
  "range": { "a1": "Data!A1:F1000" }
}
\`\`\`

Creates a new sheet with pivot table automatically!

## ğŸ›¡ï¸ SAFETY FEATURES

â€¢ User confirmation via MCP Elicitation (SEP-1036)
â€¢ Rollback support if creation fails
â€¢ Validation before modification
â€¢ Clear error messages

Ready to visualize your data! ğŸ¨`,
            },
          },
        ],
      };
    }
  );

  server.registerPrompt(
    'analyze_with_history',
    {
      description: 'ğŸ”— Reference previous analysis results via MCP Resources',
      argsSchema: {
        spreadsheetId: z.string().describe('Spreadsheet ID from URL (required)'),
      },
    },
    async (args: Record<string, unknown>) => {
      return {
        messages: [
          {
            role: 'user' as const,
            content: {
              type: 'text' as const,
              text: `ğŸ”— Analysis History via MCP Resources

Spreadsheet: ${args['spreadsheetId']}

## ğŸ“š STORED ANALYSIS RESULTS

Every successful \`analyze_data\` is automatically stored as an MCP Resource:
\`analyze://results/{id}\`

## ğŸ” AVAILABLE RESOURCES

**List all recent analyses:**
\`\`\`
Resource: analyze://results
\`\`\`

Returns:
\`\`\`json
{
  "count": 5,
  "results": [
    {
      "id": "analysis-1",
      "spreadsheetId": "${args['spreadsheetId']}",
      "timestamp": "2026-01-12T10:30:00Z",
      "summary": "Fast statistical analysis complete...",
      "uri": "analyze://results/analysis-1"
    }
  ]
}
\`\`\`

**Get specific analysis:**
\`\`\`
Resource: analyze://results/analysis-1
\`\`\`

Returns full analysis result with all findings.

## ğŸ’¬ CONVERSATIONAL WORKFLOWS

**Pattern 1: Compare with Previous**
\`\`\`
User: "Analyze Sheet1"
Assistant: [Runs analyze_data, stores as analysis-1]
          "...completed (stored as analyze://results/analysis-1)"

User: "How does this compare to last week?"
Assistant: [Reads analyze://results/analysis-1]
           "Last week's quality score was 85, now it's 92..."
\`\`\`

**Pattern 2: Explain Previous Analysis**
\`\`\`json
{
  "tool": "sheets_analyze",
  "action": "explain_analysis",
  "analysisResult": { /* from analyze://results/analysis-1 */ },
  "question": "Why did quality improve?"
}
\`\`\`

**Pattern 3: Track Quality Over Time**
\`\`\`
1. List: analyze://results
2. Filter: analyses for same spreadsheet
3. Compare: quality scores over time
4. Report: "Quality improving by 5% per week"
\`\`\`

## ğŸ“Š OTHER ANALYSIS RESOURCES

â€¢ \`analyze://stats\` - Service statistics (success rate, avg time)
â€¢ \`analyze://help\` - Full analysis documentation

## ğŸ¯ BENEFITS

âœ… No need to re-run analyses
âœ… Reference previous results in follow-up questions
âœ… Track data quality over time
âœ… Compare before/after cleanup
âœ… MCP-native (standard resource protocol)

## ğŸ’¾ STORAGE

â€¢ Last 100 analyses kept in memory
â€¢ Automatic cleanup of old results
â€¢ No manual storage required
â€¢ Access via standard MCP resource URIs

Ready to leverage analysis history! ğŸ”—`,
            },
          },
        ],
      };
    }
  );
}
